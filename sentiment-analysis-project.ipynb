## The Kaggle Notebook for training the module
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Load pre-trained DistilBERT tokenizer and model\nmodel_name = 'distilbert-base-uncased'\ntokenizer = DistilBertTokenizer.from_pretrained(model_name)\nmodel = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # Assuming three classes (positive/negative/neutral)\n\n# Define parameters for training\nbatch_size = 32\nlearning_rate = 2e-5\nnum_epochs = 3\n\n# Read data from Excel spreadsheet\ndf = pd.read_csv('E:/Programming/New Project (Sentiment analysis)/Dataset/test.csv', encoding='latin1')\nlabel_column = 'sentiment'\ndf.dropna(subset=[label_column], inplace=True)\n\n# Define label mapping\nlabel_map = {'positive': 1, 'negative': 0, 'neutral': 2}\n\n# Tokenization and processing\ntokenized_texts = []\nlabels = []\nfor i, row in df.iterrows():\n    if pd.notna(row['text']):\n        tokens = tokenizer.encode(row['text'], max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n        tokenized_texts.append(tokens)\n        labels.append(label_map[row[label_column]])\n\nX = torch.cat(tokenized_texts, dim=0)\ny = torch.tensor(labels)\n\n# Create a DataLoader for training\ntrain_dataset = TensorDataset(X, y)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Define optimizer and loss function\noptimizer = AdamW(model.parameters(), lr=learning_rate)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        inputs, labels = batch\n        optimizer.zero_grad()\n        outputs = model(inputs)[0]\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n# Save the trained model\ntorch.save(model.state_dict(), 'E:/Programming/New Project (Sentiment analysis)/Model')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}
